{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DebuggingyVisualizacion.ipynb","provenance":[{"file_id":"1rNKI6OeQ3F8AxiEYyBs2b3W2l7xMboOH","timestamp":1569613227818},{"file_id":"1H-tFOn_pLTwJOWktioAJievDf1bjo3B8","timestamp":1569597837052},{"file_id":"1PHuCmzpDlJ4du1P7PK0lH7Tam9-tBnAL","timestamp":1569596842662},{"file_id":"17U2bbvgNQKmJGwKVWGipqZxG4NyL17hj","timestamp":1569595568885},{"file_id":"1xOQslnWfEoXQeL2JYJN3whZfEECdUlv-","timestamp":1569594222572},{"file_id":"1k46pYaSKoqhcWsmopEeUuTa_P0MgSSYn","timestamp":1569592594779},{"file_id":"1A_mOBWeHcGG6TI3KuzWsHzu3U0jBKo35","timestamp":1569518342345}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"l3UAFnVkaJI7","colab_type":"code","outputId":"dfc9373d-6d3f-4525-bc5e-407796519988","executionInfo":{"status":"ok","timestamp":1569613918502,"user_tz":300,"elapsed":21210,"user":{"displayName":"Juan Pablo Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCXIpTgcVxmErvRRFgKjsm7ZvtmUCNpAXJQ3jw8=s64","userId":"14399177404402354544"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"itIn4FKWaWU7","colab_type":"code","outputId":"adbb2455-be6e-4d56-8a0e-992b25e6630e","executionInfo":{"status":"ok","timestamp":1569593063326,"user_tz":300,"elapsed":921,"user":{"displayName":"Juan Pablo Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCXIpTgcVxmErvRRFgKjsm7ZvtmUCNpAXJQ3jw8=s64","userId":"14399177404402354544"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd '/gdrive/My Drive'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/gdrive/My Drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1rXc9Cv-acj4","colab_type":"code","colab":{}},"source":["!git clone https://github.com/aitorzip/PyTorch-CycleGAN.git"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5-6tCdm6akD4","colab_type":"code","colab":{}},"source":["%cd '/gdrive/My Drive/PyTorch-CycleGAN'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ysBk4vwGarmx","colab_type":"code","colab":{}},"source":["%%sh\n","sh ./download_dataset summer2winter_yosemite"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d9GKgOZ0a01m","colab_type":"code","colab":{}},"source":["!mv datasets/summer2winter_yosemite /gdrive/My\\ Drive/dl-pytorch/datasets/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B4BK9A7ebIa4","colab_type":"code","colab":{}},"source":["!ls /gdrive/My\\ Drive/dl-pytorch/datasets/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jAbgGjA0CNy2","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class ResidualBlock(nn.Module):\n","  def __init__(self, in_features):\n","    super(ResidualBlock, self).__init__()\n","    \n","    conv_block = [ nn.ReflectionPad2d(1), #mejor padding\n","                   nn.Conv2d(in_features, in_features, 3),\n","                   nn.InstanceNorm2d(in_features), #BN para GANS\n","                   nn.ReLU(True),\n","                   nn.ReflectionPad2d(1), #mejor para consrvar distribucion\n","                   nn.Conv2d(in_features, in_features, 3),\n","                   nn.InstanceNorm2d(in_features)\n","                 ]\n","    \n","    self.conv_block = nn.Sequential(*conv_block)\n","  def forward(self, x):\n","    return self.conv_block(x) + x #una idea poderosa"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pFhVXWliCQyB","colab_type":"code","colab":{}},"source":["class Generator(nn.Module):\n","  def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n","    super(Generator,self).__init__()\n","    \n","    # Bloqueconvolucional\n","    model = [ nn.ReflectionPad2d(3),\n","            nn.Conv2d(input_nc, 64, 7), # I - 7 + 6 /1 +1 = I\n","            nn.InstanceNorm2d(64),\n","             nn.ReLU(True)\n","            ]\n","    \n","    in_features = 64\n","    out_features = in_features * 2\n","    \n","    #Encoding\n","    for _ in range(2):\n","      model += [ nn.Conv2d(in_features, out_features, 3, stride=2, padding=1), #I/2\n","                 nn.InstanceNorm2d(out_features),\n","                 nn.ReLU(True)\n","               ]\n","      in_features = out_features\n","      out_features = in_features*2\n","    #transformaciones residuales\n","    \n","    for _ in range(n_residual_blocks):\n","      model += [ResidualBlock(in_features)]\n","    \n","    #decoding\n","    \n","    out_features = in_features// 2\n","    for _ in range(2):\n","      model += [ nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1), #2I\n","                 nn.InstanceNorm2d(out_features),\n","                 nn.ReLU(inplace=True)\n","               ]\n","      in_features = out_features\n","      out_features = in_features //2\n","  \n","    #salida\n","    model += [ nn.ReflectionPad2d(3),\n","               nn.Conv2d(64, output_nc, 7), #I\n","               nn.Tanh()\n","             ]\n","      \n","    self.model = nn.Sequential(*model)\n","      \n","  def forward(self,x):\n","    return self.model(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-pKXwxsrCRxY","colab_type":"code","colab":{}},"source":["class Discriminator(nn.Module):\n","  \"PatchGAN: discrimina estilo o textura\"\n","  def __init__(self, input_nc):\n","    super(Discriminator, self).__init__()\n","    \n","    model = [ nn.Conv2d(input_nc, 64, 4, stride=2, padding=1), #I/2\n","              nn.LeakyReLU(0.2, inplace=True)\n","            ]\n","    \n","    model += [ nn.Conv2d(64, 128, 4, stride=2, padding=1), #I/2\n","               nn.InstanceNorm2d(128),\n","              nn.LeakyReLU(0.2, inplace=True)\n","             ]\n","    \n","    model += [ nn.Conv2d(128, 256, 4, stride=2, padding=1), #I/2\n","               nn.InstanceNorm2d(256),\n","              nn.LeakyReLU(0.2, inplace=True)\n","             ]\n","    \n","    model += [ nn.Conv2d(256, 512, 4, padding=1), #I-1\n","               nn.InstanceNorm2d(512),\n","              nn.LeakyReLU(0.2, inplace=True)\n","             ]\n","    \n","    # Flatten\n","    model += [nn.Conv2d(512, 1, 4, padding=1)] #I-1\n","    \n","    self.model = nn.Sequential(*model)\n","    \n","  def forward(self, x):\n","    x = self.model(x)\n","    return F.avg_pool2d(x, x.size()[2:]).view(x.size()[0], -1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4AUewAOqCf8v","colab_type":"code","colab":{}},"source":["import sys\n","sys.path.append('/gdrive/My Drive/dl-pytorch/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ly3Glz0QD7vf","colab_type":"code","colab":{}},"source":["import glob\n","import random\n","import os\n","import itertools\n","from PIL import Image\n","\n","import torch\n","\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","\n","from utils import ReplayBuffer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7k7lkmVbEUgO","colab_type":"code","colab":{}},"source":["class ImageDataset(Dataset):\n","  def __init__(self, base_dir, transform=None, split='train'):\n","    self.transform = transforms.Compose(transform)\n","    self.files_A = sorted(glob.glob(os.path.join(base_dir, '{}/A/*.*'.format(split))))\n","    self.files_B = sorted(glob.glob(os.path.join(base_dir, '{}/B/*.*'.format(split))))\n","    \n","  def __len__(self):\n","    return max(len(self.files_A), len(self.files_B))\n","  \n","  def __getitem__(self,idx):\n","    image_A = self.transform(Image.open(self.files_A[idx]))\n","    image_B = self.transform(Image.open(self.files_B[random.randint(0,len(self.files_B)-1)]))\n","    return {'A': image_A, 'B': image_B}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RWlNSvgnHFaQ","colab_type":"code","colab":{}},"source":["epoch = 0\n","n_epochs = 200\n","batch_size = 4\n","lr = 0.0002\n","size = 256\n","input_nc = 3\n","output_nc = 3\n","decay_epoch = 100 #pending\n","\n","cuda = True\n","n_cpu = 8\n","\n","base_dir = '/gdrive/My Drive/dl-pytorch/datasets/summer2winter_yosemite/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ygAUApH-JPXk","colab_type":"code","outputId":"2f730c25-36e7-4dce-a444-04310d2e5c34","executionInfo":{"status":"ok","timestamp":1569615508211,"user_tz":300,"elapsed":1635,"user":{"displayName":"Juan Pablo Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCXIpTgcVxmErvRRFgKjsm7ZvtmUCNpAXJQ3jw8=s64","userId":"14399177404402354544"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["device = torch.device('cuda' if cuda else 'cpu')\n","\n","def weights_init_normal(m):\n","  if isinstance(m, nn.Conv2d):\n","    torch.nn.init.normal(m.weight.data, 0.0, 0.02)\n","  elif isinstance(m, nn.BatchNorm2d):\n","    torch.nn.init.normal(m.weight.data, 1.0, 0.02)\n","    torch.nn.init.constant(m.bias, 0.0)\n","    \n","netG_A2B = Generator(input_nc, output_nc)\n","netG_B2A = Generator(input_nc, output_nc)\n","netD_A = Discriminator(input_nc)\n","netD_B = Discriminator(input_nc)\n","\n","netG_A2B.apply(weights_init_normal)\n","netG_B2A.apply(weights_init_normal)\n","netD_A.apply(weights_init_normal)\n","netD_B.apply(weights_init_normal)\n","\n","if cuda:\n","  netG_A2B.to(device)\n","  netG_B2A.to(device)\n","  netD_A.to(device)\n","  netD_B.to(device)\n","  \n","criterion_GAN = torch.nn.MSELoss()\n","criterion_cycle = torch.nn.L1Loss()\n","criterion_identity = torch.nn.L1Loss()\n","\n","optimizer_G = torch.optim.Adam(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()),\n","                              lr=lr, betas=(0.5, 0.999))\n","optimizer_D_A = torch.optim.Adam(netD_A.parameters(), lr=lr, betas=(0.5,0.999))\n","optimizer_D_B = torch.optim.Adam(netD_B.parameters(), lr=lr, betas=(0.5,0.999))\n","\n","#schedulers (actualizar el learning rate de forma dinamica durante el entrenamiento)\n","\n","class LambdaLR():\n","  def __init__(self, n_epochs, offset, decay_start_epoch):\n","    assert ((n_epochs - decay_start_epoch) > 0)\n","    self.n_epochs = n_epochs\n","    self.offset = offset\n","    self.decay_start_epoch = decay_start_epoch\n","    \n","  def step(self, epoch):\n","    return 1 - max(0, epoch + self.offset - self.decay_start_epoch)/(self.n_epochs - self.decay_start_epoch)\n","\n","lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=LambdaLR(n_epochs,epoch,decay_epoch).step)\n","lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=LambdaLR(n_epochs,epoch,decay_epoch).step)\n","lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=LambdaLR(n_epochs,epoch,decay_epoch).step)"],"execution_count":42,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n","  \"\"\"\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"DWP_N0oURPqp","colab_type":"code","colab":{}},"source":["#inputs y targets\n","\n","Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n","target_real = Tensor(batch_size).fill_(1.0)\n","target_fake = Tensor(batch_size).fill_(0.0)\n","\n","fake_A_buffer = ReplayBuffer()\n","fake_B_buffer = ReplayBuffer()\n","\n","#Dataloader\n","\n","transform = [ transforms.Resize(int(size*1.12), Image.BICUBIC),\n","              transforms.RandomCrop(size),\n","             transforms.RandomHorizontalFlip(),\n","             transforms.ToTensor(),\n","             transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n","            ]\n","\n","dataloader = DataLoader(ImageDataset(base_dir,transform=transform),\n","                       batch_size=batch_size, shuffle=True, num_workers=n_cpu, drop_last=True)\n","\n","def Gen_GAN_loss(G, D, real, loss, target_real):\n","  fake = G(real)\n","  pred_fake = D(fake)\n","  L = loss(pred_fake, target_real)\n","  return L, fake\n","\n","def cycle_loss(G1, G2, real, loss):\n","  recovered = G2(G1(real))\n","  L = loss(recovered, real)\n","  return L\n","\n","def identity_loss(G, real, loss):\n","  same = G(real)\n","  L = loss(same,real)\n","  return L\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1XMD7vRyOlW_","colab_type":"code","colab":{}},"source":["for epoch in range(epoch, n_epochs):\n","  for i, batch in enumerate(dataloader):\n","    real_A = batch['A'].to(device)\n","    real_B = batch['B'].to(device)\n","    \n","    # Generativas\n","    optimizer_G.zero_grad()\n","    \n","    loss_GAN_A2B, fake_B = Gen_GAN_loss(netG_A2B, netD_B, real_A, criterion_GAN, target_real)\n","    loss_GAN_B2A, fake_A = Gen_GAN_loss(netG_B2A, netD_A, real_B, criterion_GAN, target_real)\n","    \n","    loss_cycle_ABA = cycle_loss(netG_A2B, netG_B2A, real_A, criterion_cycle)\n","    loss_cycle_BAB = cycle_loss(netG_B2A, netG_A2B, real_B, criterion_cycle)\n","    \n","    loss_identity_A = identity_loss(netG_B2A, real_A, criterion_identity)\n","    loss_identity_B = identity_loss(netG_A2B, real_B, criterion_identity)\n","    \n","    loss_G = (loss_GAN_A2B + loss_GAN_B2A) + 10.0*(loss_cycle_ABA + loss_cycle_BAB) + 5.0 *(loss_identity_A + loss_identity_B)\n","    loss_G.backward()\n","    \n","    optimizer_G.step\n","    \n","    #Discriminativas\n","    optimizer_D_A.zero_grad()\n","    \n","    loss_D_A = Disc_GAN_loss(netD_A, fake_A, real_A, fake_A_buffer, criterion_GAN, target_real, target_fake)\n","    loss_D_A.backward()\n","    optimizer_D_A.step()\n","    \n","    optimizer_D_B.zero_grad()\n","    \n","    loss_D_B = Disc_GAN_loss(netD_B, fake_B, real_B, fake_B_buffer, criterion_GAN, target_real, target_fake)\n","    loss_D_B.backward()\n","    optimizer_D_B.step()\n","    \n","  lr_scheduler_G.step()\n","  lr_scheduler_D_A.step()\n","  lr_scheduler_D_B.step()\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qDdbFCeH7ju_","colab_type":"code","colab":{}},"source":["import sys\n","import os\n","\n","import torchvision.transforms as transforms\n","from torchvision.utils import save_image\n","from torch.utils.data import DataLoader\n","from torch.autograd import Variable\n","import torch\n","\n","\n","from datasets import ImageDataset\n","\n","generator_A2B='output/netG_A2B.pth'\n","generator_B2A='output/netG_B2A.pth'\n","\n","###### Definition of variables ######\n","# Networks\n","netG_A2B = Generator(input_nc, output_nc)\n","netG_B2A = Generator(output_nc, input_nc)\n","\n","if cuda:\n","    netG_A2B.cuda()\n","    netG_B2A.cuda()\n","\n","# Load state dicts\n","netG_A2B.load_state_dict(torch.load(generator_A2B))\n","netG_B2A.load_state_dict(torch.load(generator_B2A))\n","\n","# Set model's test mode\n","netG_A2B.eval()\n","netG_B2A.eval()\n","\n","# Inputs & targets memory allocation\n","Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n","input_A = Tensor(batchSize, input_nc, size, size)\n","input_B = Tensor(batchSize, output_nc, size, size)\n","\n","# Dataset loader\n","transforms_ = [ transforms.ToTensor(),\n","                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ]\n","dataloader = DataLoader(ImageDataset(dataroot, transforms_=transforms_, mode='test'), \n","                        batch_size=batchSize, shuffle=False, num_workers=n_cpu, drop_last=True)\n","###################################\n","\n","###### Testing######\n","\n","# Create output dirs if they don't exist\n","if not os.path.exists('output/A'):\n","    os.makedirs('output/A')\n","if not os.path.exists('output/B'):\n","    os.makedirs('output/B')\n","\n","for i, batch in enumerate(dataloader):\n","    # Set model input\n","    real_A = Variable(input_A.copy_(batch['A']))\n","    real_B = Variable(input_B.copy_(batch['B']))\n","\n","    # Generate output\n","    fake_B = 0.5*(netG_A2B(real_A).data + 1.0)\n","    fake_A = 0.5*(netG_B2A(real_B).data + 1.0)\n","\n","    # Save image files\n","    save_image(fake_A, 'output/A/%04d.png' % (i+1))\n","    save_image(fake_B, 'output/B/%04d.png' % (i+1))\n","\n","    sys.stdout.write('\\rGenerated images %04d of %04d' % (i+1, len(dataloader)))\n","\n","sys.stdout.write('\\n')\n","###################################"],"execution_count":0,"outputs":[]}]}